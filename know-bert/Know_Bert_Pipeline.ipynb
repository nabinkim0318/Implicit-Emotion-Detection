{"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"VYrqNTmG0_AU"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39970,"status":"ok","timestamp":1733710531863,"user":{"displayName":"Feyi Ogunsanya","userId":"17897576881381240417"},"user_tz":300},"id":"4hNEt-KGCaVX","outputId":"dd8d2b9b-9c2a-4d1a-bf01-6a80ac30a480"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-3.1.0-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.46.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cpu)\n","Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Collecting allennlp\n","  Downloading allennlp-2.10.1-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n","Collecting xxhash (from datasets)\n","  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n","Collecting fsspec<=2024.9.0,>=2023.1.0 (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets)\n","  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n","Collecting aiohttp (from datasets)\n","  Downloading aiohttp-3.11.10-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n","Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.26.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Collecting torch\n","  Downloading torch-1.12.1-cp310-cp310-manylinux1_x86_64.whl.metadata (22 kB)\n","Collecting torchvision<0.14.0,>=0.8.1 (from allennlp)\n","  Downloading torchvision-0.13.1-cp310-cp310-manylinux1_x86_64.whl.metadata (10 kB)\n","Collecting cached-path<1.2.0,>=1.1.3 (from allennlp)\n","  Downloading cached_path-1.1.6-py3-none-any.whl.metadata (6.0 kB)\n","Collecting fairscale==0.4.6 (from allennlp)\n","  Downloading fairscale-0.4.6.tar.gz (248 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m248.2/248.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.9.1)\n","Collecting spacy<3.4,>=2.1.0 (from allennlp)\n","  Downloading spacy-3.3.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n","Collecting tensorboardX>=1.2 (from allennlp)\n","  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n","Requirement already satisfied: h5py>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from allennlp) (3.12.1)\n","Requirement already satisfied: scikit-learn>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.5.2)\n","Requirement already satisfied: scipy>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from allennlp) (1.13.1)\n","Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.10/dist-packages (from allennlp) (8.3.4)\n","Collecting transformers\n","  Downloading transformers-4.20.1-py3-none-any.whl.metadata (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.3/77.3 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentencepiece>=0.1.96 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.2.0)\n","Collecting filelock (from datasets)\n","  Downloading filelock-3.7.1-py3-none-any.whl.metadata (2.5 kB)\n","Collecting lmdb>=1.2.1 (from allennlp)\n","  Downloading lmdb-1.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n","Collecting more-itertools>=8.12.0 (from allennlp)\n","  Downloading more_itertools-10.5.0-py3-none-any.whl.metadata (36 kB)\n","Collecting termcolor==1.1.0 (from allennlp)\n","  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting wandb<0.13.0,>=0.10.0 (from allennlp)\n","  Downloading wandb-0.12.21-py2.py3-none-any.whl.metadata (7.2 kB)\n","Collecting base58>=2.1.1 (from allennlp)\n","  Downloading base58-2.1.1-py3-none-any.whl.metadata (3.1 kB)\n","Collecting sacremoses (from allennlp)\n","  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: typer>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (0.15.0)\n","Collecting protobuf<4.0.0,>=3.12.0 (from allennlp)\n","  Downloading protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n","Requirement already satisfied: traitlets>5.1.1 in /usr/local/lib/python3.10/dist-packages (from allennlp) (5.7.1)\n","Collecting jsonnet>=0.10.0 (from allennlp)\n","  Downloading jsonnet-0.20.0.tar.gz (594 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.2/594.2 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers)\n","  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.5 kB)\n","Collecting rich<13.0,>=12.1 (from cached-path<1.2.0,>=1.1.3->allennlp)\n","  Downloading rich-12.6.0-py3-none-any.whl.metadata (18 kB)\n","Collecting boto3<2.0,>=1.0 (from cached-path<1.2.0,>=1.1.3->allennlp)\n","  Downloading boto3-1.35.76-py3-none-any.whl.metadata (6.7 kB)\n","Requirement already satisfied: google-cloud-storage<3.0,>=1.32.0 in /usr/local/lib/python3.10/dist-packages (from cached-path<1.2.0,>=1.1.3->allennlp) (2.8.0)\n","INFO: pip is looking at multiple versions of cached-path to determine which version is compatible with other requirements. This could take a while.\n","Collecting cached-path<1.2.0,>=1.1.3 (from allennlp)\n","  Downloading cached_path-1.1.5-py3-none-any.whl.metadata (6.0 kB)\n","  Downloading cached_path-1.1.4-py3-none-any.whl.metadata (6.0 kB)\n","  Downloading cached_path-1.1.3-py3-none-any.whl.metadata (6.0 kB)\n","Collecting transformers\n","  Downloading transformers-4.20.0-py3-none-any.whl.metadata (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.3/77.3 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: pip is still looking at multiple versions of cached-path to determine which version is compatible with other requirements. This could take a while.\n","  Downloading transformers-4.19.4-py3-none-any.whl.metadata (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.19.3-py3-none-any.whl.metadata (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","  Downloading transformers-4.19.2-py3-none-any.whl.metadata (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.19.1-py3-none-any.whl.metadata (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.19.0-py3-none-any.whl.metadata (73 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.18.0-py3-none-any.whl.metadata (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.3/70.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.17.0-py3-none-any.whl.metadata (67 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.16.2-py3-none-any.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.16.1-py3-none-any.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.16.0-py3-none-any.whl.metadata (61 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.8/61.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.15.0-py3-none-any.whl.metadata (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.8/59.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers<0.11,>=0.10.1 (from transformers)\n","  Downloading tokenizers-0.10.3.tar.gz (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.7/212.7 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting transformers\n","  Downloading transformers-4.14.1-py3-none-any.whl.metadata (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m243.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.13.0-py3-none-any.whl.metadata (59 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.7/59.7 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.12.5-py3-none-any.whl.metadata (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.12.4-py3-none-any.whl.metadata (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.12.3-py3-none-any.whl.metadata (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.12.2-py3-none-any.whl.metadata (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.12.1-py3-none-any.whl.metadata (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.12.0-py3-none-any.whl.metadata (56 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.11.3-py3-none-any.whl.metadata (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.11.2-py3-none-any.whl.metadata (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.11.1-py3-none-any.whl.metadata (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.11.0-py3-none-any.whl.metadata (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.7/53.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.10.3-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.10.2-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.10.1-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.10.0-py3-none-any.whl.metadata (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.9.2-py3-none-any.whl.metadata (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n","  Downloading transformers-4.9.1-py3-none-any.whl.metadata (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.9.0-py3-none-any.whl.metadata (49 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.8.2-py3-none-any.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.8.1-py3-none-any.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.8.0-py3-none-any.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.8/48.8 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.7.0-py3-none-any.whl.metadata (48 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.3/48.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.6.1-py3-none-any.whl.metadata (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hINFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n","  Downloading transformers-4.6.0-py3-none-any.whl.metadata (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.4/45.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Downloading transformers-4.5.1-py3-none-any.whl.metadata (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting allennlp\n","  Downloading allennlp-2.10.0-py3-none-any.whl.metadata (20 kB)\n","Collecting torch\n","  Downloading torch-1.11.0-cp310-cp310-manylinux1_x86_64.whl.metadata (24 kB)\n","Collecting torchvision<0.13.0,>=0.8.1 (from allennlp)\n","  Downloading torchvision-0.12.0-cp310-cp310-manylinux1_x86_64.whl.metadata (10 kB)\n","Collecting rich==12.1 (from allennlp)\n","  Downloading rich-12.1.0-py3-none-any.whl.metadata (19 kB)\n","Collecting protobuf==3.20.0 (from allennlp)\n","  Downloading protobuf-3.20.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (698 bytes)\n","Collecting commonmark<0.10.0,>=0.9.0 (from rich==12.1->allennlp)\n","  Downloading commonmark-0.9.1-py2.py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: pygments<3.0.0,>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from rich==12.1->allennlp) (2.18.0)\n","Collecting allennlp\n","  Downloading allennlp-2.9.3-py3-none-any.whl.metadata (19 kB)\n","Collecting spacy<3.3,>=2.1.0 (from allennlp)\n","  Downloading spacy-3.2.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n","Collecting filelock (from datasets)\n","  Downloading filelock-3.6.0-py3-none-any.whl.metadata (2.6 kB)\n","Requirement already satisfied: more-itertools in /usr/lib/python3/dist-packages (from allennlp) (8.10.0)\n","Collecting cached-path<1.2.0,>=1.0.2 (from allennlp)\n","  Downloading cached_path-1.1.2-py3-none-any.whl.metadata (6.0 kB)\n","  Downloading cached_path-1.1.1-py3-none-any.whl.metadata (6.0 kB)\n","  Downloading cached_path-1.1.0-py3-none-any.whl.metadata (6.0 kB)\n","  Downloading cached_path-1.0.2-py3-none-any.whl.metadata (6.0 kB)\n","Collecting allennlp\n","  Downloading allennlp-2.9.2-py3-none-any.whl.metadata (19 kB)\n","  Downloading allennlp-2.9.1-py3-none-any.whl.metadata (19 kB)\n","INFO: pip is looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.\n","  Downloading allennlp-2.9.0-py3-none-any.whl.metadata (18 kB)\n","  Downloading allennlp-2.8.0-py3-none-any.whl.metadata (17 kB)\n","  Downloading allennlp-2.7.0-py3-none-any.whl.metadata (17 kB)\n","  Downloading allennlp-2.6.0-py3-none-any.whl.metadata (17 kB)\n","  Downloading allennlp-2.5.0-py3-none-any.whl.metadata (17 kB)\n","  Downloading allennlp-2.4.0-py3-none-any.whl.metadata (17 kB)\n","  Downloading allennlp-2.3.1-py3-none-any.whl.metadata (17 kB)\n","INFO: pip is still looking at multiple versions of allennlp to determine which version is compatible with other requirements. This could take a while.\n","  Downloading allennlp-2.3.0-py3-none-any.whl.metadata (17 kB)\n","  Downloading allennlp-2.2.0-py3-none-any.whl.metadata (17 kB)\n","  Downloading allennlp-2.1.0-py3-none-any.whl.metadata (17 kB)\n","  Downloading allennlp-2.0.1-py3-none-any.whl.metadata (15 kB)\n","  Downloading allennlp-2.0.0-py3-none-any.whl.metadata (15 kB)\n","INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n","  Downloading allennlp-1.5.0-py3-none-any.whl.metadata (15 kB)\n","  Downloading allennlp-1.4.1-py3-none-any.whl.metadata (15 kB)\n","  Downloading allennlp-1.4.0-py3-none-any.whl.metadata (15 kB)\n","  Downloading allennlp-1.3.0-py3-none-any.whl.metadata (15 kB)\n","  Downloading allennlp-1.2.2-py3-none-any.whl.metadata (15 kB)\n","  Downloading allennlp-1.2.1-py3-none-any.whl.metadata (14 kB)\n","  Downloading allennlp-1.2.0-py3-none-any.whl.metadata (13 kB)\n","  Downloading allennlp-1.1.0-py3-none-any.whl.metadata (13 kB)\n","  Downloading allennlp-1.0.0-py3-none-any.whl.metadata (11 kB)\n","  Downloading allennlp-0.9.0-py3-none-any.whl.metadata (11 kB)\n","Collecting overrides (from allennlp)\n","  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n","Collecting spacy<2.2,>=2.1.0 (from allennlp)\n","  Downloading spacy-2.1.9.tar.gz (30.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","  \n","  \u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n","  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","  \u001b[31m╰─>\u001b[0m See above for output.\n","  \n","  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","  Installing build dependencies ... \u001b[?25l\u001b[?25herror\n","\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n","\n","\u001b[31m×\u001b[0m \u001b[32mpip subprocess to install build dependencies\u001b[0m did not run successfully.\n","\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n","\u001b[31m╰─>\u001b[0m See above for output.\n","\n","\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n","Collecting huggingface-hub==0.24.0\n","  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.24.0) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.24.0) (2024.10.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.24.0) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.24.0) (6.0.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.24.0) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.24.0) (4.66.6)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub==0.24.0) (4.12.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.24.0) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.24.0) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.24.0) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub==0.24.0) (2024.8.30)\n","Downloading huggingface_hub-0.24.0-py3-none-any.whl (419 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m419.0/419.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: huggingface-hub\n","  Attempting uninstall: huggingface-hub\n","    Found existing installation: huggingface-hub 0.26.3\n","    Uninstalling huggingface-hub-0.26.3:\n","      Successfully uninstalled huggingface-hub-0.26.3\n","Successfully installed huggingface-hub-0.24.0\n"]}],"source":["!pip install datasets transformers torch datasets evaluate allennlp\n","!pip install --upgrade huggingface-hub==0.24.0\n","from huggingface_hub import split_torch_state_dict_into_shards\n","!pip uninstall -y torch transformers\n","!pip install allennlp==2.10.1\n","!pip install evaluate datasets transformers\n","!pip install pytorch_pretrained_bert\n","!pip install datasets\n","from google.colab import drive\n","drive.mount('/content/drive')\n","import os,sys, allennlp\n","import knowbert\n","import model\n","os.chdir('/content/drive/My Drive/Gatech courses/Fall 2024 Courses/Natural Language Processing/NLP Project/Code/knowbert')\n","#os.chdir('/content/drive/MyDrive/Uni/Semesters/2024 Fall/CS 7650 (NLP)/NLP Project')"]},{"cell_type":"code","source":["import os\n","import torch\n","import pandas as pd\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","from nltk import word_tokenize\n","import nltk\n","from typing import Union, List\n","from datasets import Dataset, DatasetDict\n","from tqdm import tqdm\n","from google.colab import drive\n","import requests\n","import tarfile\n","import tempfile\n","import torch\n","from typing import Union\n","import logging\n","import shutil\n","from pathlib import Path\n","#from allennlp.models.archival import load_archive\n","# from allennlp.common import Params\n","# from allennlp.data import Instance, Vocabulary\n","# from allennlp.data.data_loaders import MultiProcessDataLoader  # this is the new import for DataIterator\n","# from allennlp.common.file_utils import cached_path\n","\n","#from kb.knowbert_utils import KnowBertBatchifier\n","from typing import List, Tuple, Dict, Optional\n","import logging\n","\n","# Set up logging\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(__name__)\n","\n","class KnowBERTProcessor:\n","    def __init__(self, root_path: str, batch_size: int = 32):\n","        \"\"\"\n","        Initialize KnowBERT processor with paths and configurations\n","\n","        Args:\n","            root_path: Base path for all data files\n","            batch_size: Batch size for processing\n","        \"\"\"\n","        self.root_path = root_path\n","        self.batch_size = batch_size\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.model = None\n","        self.batcher = None\n","\n","        # Download NLTK data\n","        nltk.download('punkt', quiet=True)\n","\n","        logger.info(f\"Initialized KnowBERTProcessor using device: {self.device}\")\n","\n","    def load_datasets(self, isear_path: str, conceptnet_path: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n","        \"\"\"\n","        Load ISEAR and ConceptNet datasets\n","\n","        Args:\n","            isear_path: Path to ISEAR dataset\n","            conceptnet_path: Path to ConceptNet dataset\n","\n","        Returns:\n","            Tuple of (isear_df, conceptnet_df)\n","        \"\"\"\n","        full_isear_path = os.path.join(self.root_path, isear_path)\n","        full_conceptnet_path = os.path.join(self.root_path, conceptnet_path)\n","\n","        # Verify paths exist\n","        if not os.path.exists(full_isear_path):\n","            raise FileNotFoundError(f\"ISEAR dataset not found at: {full_isear_path}\")\n","        if not os.path.exists(full_conceptnet_path):\n","            raise FileNotFoundError(f\"ConceptNet dataset not found at: {full_conceptnet_path}\")\n","\n","        # Load datasets\n","        isear_df = pd.read_csv(full_isear_path)\n","        conceptnet_df = pd.read_csv( # returns an iterator\n","            full_conceptnet_path,\n","            sep='\\t',\n","            usecols=[2, 3, 4],\n","            names=['start', 'rel', 'end'],\n","            compression='gzip',\n","            engine='python',\n","            chunksize=10000\n","        )\n","        print(f\"Loaded ISEAR dataset with {len(isear_df)} rows\")\n","        print(f\"Each loaded ConceptNet chunk {len(next(conceptnet_df))} rows\")\n","        return isear_df, conceptnet_df\n","\n","    def download_model(url: str, save_path: str = 'model.tar.gz') -> str:\n","      \"\"\"Download the model archive from a URL if not already downloaded.\"\"\"\n","      if not os.path.exists(save_path):\n","          response = requests.get(url, stream=True)\n","          with open(save_path, 'wb') as file:\n","              for chunk in response.iter_content(chunk_size=1024):\n","                  if chunk:\n","                      file.write(chunk)\n","      return save_path\n","\n","    def load_archive_helper(\n","      self,\n","      archive_file: Union[str, os.PathLike],\n","      cuda_device: int = -1,\n","      weights_file: str = None\n","  ) -> torch.nn.Module:\n","      \"\"\"\n","      This function handles the loading of a PyTorch model from a tar.gz archive file that contains model weights (and possibly other associated files like configuration files\n","\n","      Parameters:\n","      - archive_file (Union[str, os.PathLike]): Path to the archive file.\n","      - cuda_device (int): Device to load the model on. `-1` for CPU, >=0 for GPU device.\n","      - weights_file (str, optional): Specific weights file to use from the archive if specified.\n","\n","      Returns:\n","      - model (torch.nn.Module): Loaded PyTorch model.\n","      \"\"\"\n","      if not os.path.exists(archive_file):\n","          raise FileNotFoundError(f\"Archive file not found: {archive_file}\")\n","\n","      tempdir = None\n","      try:\n","          tempdir = tempfile.mkdtemp()\n","          print(f\"Extracting archive file {archive_file} to temporary directory {tempdir}\")\n","          with tarfile.open(archive_file, \"r:gz\") as archive:\n","              archive.extractall(tempdir)\n","\n","          weights_path = weights_file or os.path.join(tempdir, 'weights.th')\n","          if not os.path.exists(weights_path):\n","              raise FileNotFoundError(f\"Expected weights file not found in archive: {weights_path}\")\n","\n","          device = torch.device(\"cuda\" if cuda_device >= 0 and torch.cuda.is_available() else \"cpu\")\n","          model = torch.load(weights_path, map_location=device)\n","          model.eval()\n","          print(f\"Successfully loaded model from {weights_path} onto {device}\")\n","          return model\n","      finally:\n","          if tempdir is not None:\n","              print(f\"Removing temporary directory {tempdir}\")\n","              shutil.rmtree(tempdir, ignore_errors=True)\n","\n","    def load_archive(self,archive_file: Union[str, os.PathLike], cuda_device: int = -1) -> torch.nn.Module:\n","      \"\"\"\n","      Load a PyTorch knowbert model from a `tar.gz` archive file.\n","\n","      Parameters:\n","      - archive_file (Union[str, os.PathLike]): Path to the archive file.\n","      - cuda_device (int): Device to load the model on. `-1` for CPU, >=0 for GPU device.\n","\n","      Returns:\n","      - model (torch.nn.Module): Loaded PyTorch model.\n","      \"\"\"\n","      if archive_file.startswith(\"http://\") or archive_file.startswith(\"https://\"):\n","        local_archive_file = 'model_download.tar.gz'\n","        if not os.path.exists(local_archive_file):\n","            response = requests.get(archive_file, stream=True)\n","            with open(local_archive_file, 'wb') as f:\n","                for chunk in response.iter_content(chunk_size=1024):\n","                    if chunk:\n","                        f.write(chunk)\n","      else:\n","          local_archive_file = archive_file\n","\n","      tempdir = None\n","      try:\n","          tempdir = tempfile.mkdtemp()\n","          print(f\"Extracting archive file {local_archive_file} to temporary directory {tempdir}\")\n","          with tarfile.open(local_archive_file, \"r:gz\") as archive:\n","              archive.extractall(tempdir)\n","              extracted_files = os.listdir(tempdir)\n","              print(f\"Extracted files: {extracted_files}\")\n","\n","          weights_file = os.path.join(tempdir, 'weights.th')\n","          if not os.path.exists(weights_file):\n","              raise FileNotFoundError(f\"Expected weights file not found in archive: {weights_file}\")\n","\n","          config_path = os.path.join(tempdir, 'config.json')\n","          if not os.path.exists(config_path):\n","              raise FileNotFoundError(f\"Expected config file not found in archive: {config_path}\")\n","\n","          archive = self.load_archive_helper(\n","              local_archive_file,\n","              cuda_device,\n","              weights_file\n","          )\n","\n","          return archive\n","      finally:\n","          if tempdir is not None:\n","              logger.info(f\"Removing temporary directory {tempdir}\")\n","              shutil.rmtree(tempdir, ignore_errors=True)\n","\n","    def initialize_knowbert(self, model_url: str = 'https://allennlp.s3-us-west-2.amazonaws.com/knowbert/models/knowbert_wiki_wordnet_model.tar.gz'):\n","        \"\"\"\n","        Initialize pre-trained KnowBERT model hosted on AllenNLP's S3 bucket, pretrained on Wikipedia and WordNet knowledge and batchifier\n","\n","        Args:\n","            model_url: URL for the KnowBERT model\n","        \"\"\"\n","        try:\n","            archive = self.load_archive(model_url)\n","            self.model = archive.model.to(self.device)\n","            self.batcher = KnowBertBatchifier(model_url)\n","            self.model.eval()\n","            logger.info(\"Successfully initialized KnowBERT model\")\n","        except Exception as e:\n","            logger.error(f\"Error initializing KnowBERT: {str(e)}\")\n","            raise\n","\n","    def get_conceptnet_triples(self, entity: str, conceptnet_df: pd.DataFrame) -> List[Tuple[str, str]]:\n","        \"\"\"\n","        Get relevant triples from ConceptNet for a given entity\n","\n","        Args:\n","            entity: Entity to look up\n","            conceptnet_df: ConceptNet DataFrame\n","\n","        Returns:\n","            List of (relation, end_entity) tuples\n","        \"\"\"\n","        entity = entity.lower().replace(\" \", \"_\")\n","        triples = conceptnet_df[conceptnet_df['start'].str.endswith(f\"/c/en/{entity}\")][['rel', 'end']]\n","        return [(row['rel'].split('/')[-1], row['end'].split('/')[-1])\n","                for _, row in triples.iterrows()]\n","\n","    def augment_sentence(self, sentence: str, conceptnet_df: pd.DataFrame, max_length: int = 512) -> str:\n","        \"\"\"\n","        Augment a sentence with ConceptNet knowledge, by adding related conceptual knowledge to it.\n","\n","        Args:\n","            sentence: Input sentence\n","            conceptnet_df: ConceptNet DataFrame\n","            max_length: Maximum length of augmented sentence\n","\n","        Returns:\n","            Augmented sentence\n","        \"\"\"\n","        entities = word_tokenize(sentence)\n","        augmented_sentence = sentence\n","\n","        for entity in entities:\n","            triples = self.get_conceptnet_triples(entity, conceptnet_df)\n","            for rel, end in triples:\n","                augmented_sentence += f\" {rel} {end}\"\n","\n","        return augmented_sentence[:max_length]\n","\n","    def process_batch(self, sentences: List[str]) -> np.ndarray:\n","        \"\"\"\n","        Process a batch of sentences through KnowBERT,  converts a group of sentences into embeddings using the KnowBERT model\n","\n","        Args:\n","            sentences: List of sentences to process\n","\n","        Returns:\n","            NumPy array of embeddings\n","        \"\"\"\n","        if self.model is None or self.batcher is None:\n","            raise ValueError(\"KnowBERT model not initialized. Call initialize_knowbert first.\")\n","\n","        with torch.no_grad():\n","            batch = self.batcher.batchify(sentences)\n","            model_output = self.model(**batch.to(self.device))\n","            embeddings = model_output['contextual_embeddings']\n","            return embeddings.cpu().numpy()\n","\n","    def process_dataset(self, df: pd.DataFrame, conceptnet_df: pd.DataFrame,\n","                       max_length: int = 512, save_path: Optional[str] = None) -> np.ndarray:\n","        \"\"\"\n","        Process entire dataset with batching\n","\n","        Args:\n","            df: Input DataFrame\n","            conceptnet_df: ConceptNet DataFrame\n","            max_length: Maximum sentence length\n","            save_path: Optional path to save embeddings\n","\n","        Returns:\n","            NumPy array of embeddings\n","        \"\"\"\n","        # Augment sentences\n","        logger.info(\"Augmenting sentences with ConceptNet knowledge...\")\n","        augmented_sentences = [\n","            self.augment_sentence(sentence, conceptnet_df, max_length)\n","            for sentence in tqdm(df['content'], desc=\"Augmenting sentences\")\n","        ]\n","\n","        # Process in batches\n","        embeddings_list = []\n","        for i in tqdm(range(0, len(augmented_sentences), self.batch_size), desc=\"Processing batches\"):\n","            batch_sentences = augmented_sentences[i:i + self.batch_size]\n","            batch_embeddings = self.process_batch(batch_sentences)\n","            embeddings_list.append(batch_embeddings)\n","\n","        # Combine embeddings\n","        all_embeddings = np.vstack(embeddings_list)\n","        logger.info(f\"Generated embeddings with shape: {all_embeddings.shape}\")\n","\n","        # Save if path provided\n","        if save_path:\n","            full_save_path = os.path.join(self.root_path, save_path)\n","            np.save(full_save_path, all_embeddings)\n","            logger.info(f\"Saved embeddings to: {full_save_path}\")\n","        return all_embeddings\n","\n","\n","class KnowBertBatchifier:\n","    \"\"\"\n","    Takes a list of sentence strings and returns a tensor dict usable with\n","    a KnowBert model\n","    \"\"\"\n","    def __init__(self, model_archive, batch_size=32,\n","                       masking_strategy=None,\n","                       wordnet_entity_file=None, vocab_dir=None):\n","\n","        config = _extract_config_from_archive(cached_path(model_archive))\n","\n","        candidate_generator_params = _find_key(\n","            config['dataset_reader'].as_dict(), 'tokenizer_and_candidate_generator'\n","        )\n","\n","        if wordnet_entity_file is not None:\n","            candidate_generator_params['entity_candidate_generators']['wordnet']['entity_file'] = wordnet_entity_file\n","\n","        self.tokenizer_and_candidate_generator = TokenizerAndCandidateGenerator.\\\n","                from_params(Params(candidate_generator_params))\n","        self.tokenizer_and_candidate_generator.whitespace_tokenize = False\n","\n","        assert masking_strategy is None or masking_strategy == 'full_mask'\n","        self.masking_strategy = masking_strategy\n","\n","        if vocab_dir is not None:\n","            vocab_params = Params({\"directory_path\": vocab_dir})\n","        else:\n","            vocab_params = config['vocabulary']\n","        self.vocab = Vocabulary.from_params(vocab_params)\n","\n","        self.iterator = DataIterator.from_params(\n","            Params({\"type\": \"basic\", \"batch_size\": batch_size})\n","        )\n","        self.iterator.index_with(self.vocab)\n","\n","    def _replace_mask(self, s):\n","        return s.replace('[MASK]', ' [MASK] ')\n","\n","    def iter_batches(self, sentences_or_sentence_pairs: Union[List[str], List[List[str]]], verbose=True):\n","        instances = []\n","        for sentence_or_sentence_pair in sentences_or_sentence_pairs:\n","            if isinstance(sentence_or_sentence_pair, list):\n","                assert len(sentence_or_sentence_pair) == 2\n","                tokens_candidates = self.tokenizer_and_candidate_generator.\\\n","                        tokenize_and_generate_candidates(\n","                                self._replace_mask(sentence_or_sentence_pair[0]),\n","                                self._replace_mask(sentence_or_sentence_pair[1]))\n","            else:\n","                tokens_candidates = self.tokenizer_and_candidate_generator.\\\n","                        tokenize_and_generate_candidates(self._replace_mask(sentence_or_sentence_pair))\n","\n","            if verbose:\n","                print(self._replace_mask(sentence_or_sentence_pair))\n","                print(tokens_candidates['tokens'])\n","\n","            if self.masking_strategy == 'full_mask':\n","                masked_indices = [index for index, token in enumerate(tokens_candidates['tokens'])\n","                      if token == '[MASK]']\n","\n","                spans_to_mask = set([(i, i) for i in masked_indices])\n","                replace_candidates_with_mask_entity(\n","                        tokens_candidates['candidates'], spans_to_mask\n","                )\n","\n","                for key in tokens_candidates['candidates'].keys():\n","                    for span_to_mask in spans_to_mask:\n","                        found = False\n","                        for span in tokens_candidates['candidates'][key]['candidate_spans']:\n","                            if tuple(span) == tuple(span_to_mask):\n","                                found = True\n","                        if not found:\n","                            tokens_candidates['candidates'][key]['candidate_spans'].append(list(span_to_mask))\n","                            tokens_candidates['candidates'][key]['candidate_entities'].append(['@@MASK@@'])\n","                            tokens_candidates['candidates'][key]['candidate_entity_priors'].append([1.0])\n","                            tokens_candidates['candidates'][key]['candidate_segment_ids'].append(0)\n","                            assert not isinstance(sentence_or_sentence_pair, list)\n","\n","\n","            fields = self.tokenizer_and_candidate_generator.\\\n","                convert_tokens_candidates_to_fields(tokens_candidates)\n","\n","            instances.append(Instance(fields))\n","\n","\n","        for batch in self.iterator(instances, num_epochs=1, shuffle=False):\n","            yield batch\n","\n","\n","processor = KnowBERTProcessor(root_path='/content/drive/My Drive/Gatech courses/Fall 2024 Courses/Natural Language Processing/NLP Project')\n","\n","try:\n","\n","    isear_df, conceptnet_df = processor.load_datasets(\n","        isear_path='Datasets/cleaned_full_isear_data_with_sentiment.csv',\n","        conceptnet_path='Knowledge_graph/conceptnet-assertions-5.7.0.csv.gz'\n","    )\n","\n","    processor.initialize_knowbert()\n","\n","    embeddings = processor.process_dataset(\n","        df=isear_df.head(10),\n","        conceptnet_df=conceptnet_df,\n","        save_path=\"knowbert_embeddings.npy\"\n","    )\n","\n","except Exception as e:\n","    logger.error(f\"Error during processing: {str(e)}\")\n","    raise\n","\n"],"metadata":{"id":"KNRlK5VSv-LZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Feyi\n","isear_path='Datasets/cleaned_full_isear_data_with_sentiment.csv'\n","root_path='/content/drive/My Drive/Gatech courses/Fall 2024 Courses/Natural Language Processing/NLP Project'\n","full_isear_path = os.path.join(root_path, isear_path)\n","df = pd.read_csv(full_isear_path, nrows=1)\n","print(print(df['content'].tolist()))"],"metadata":{"id":"iAKePrzm2ObW"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"TPU","colab":{"gpuType":"V28","machine_shape":"hm","provenance":[{"file_id":"1-J85q1Yfx9m12AAv16av8YGor0BNdoOe","timestamp":1731807874823}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}